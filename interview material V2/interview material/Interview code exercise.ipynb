{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactoring code exercise\n",
    "\n",
    "We think that code refactoring is the most effective way to show the knowledge and way of thinking of a developer, rather than coding anything from zero. \n",
    "\n",
    "Therefore, we provide in this notebook with a code that should be refactored to obtain a final output similar to the one show here. \n",
    "\n",
    "Along the notebook, we will state for some explicit rules on how the code should be developed.\n",
    "for the code refactoring, \n",
    "\n",
    "**About the task** We provide a *csv* file resulted from an optical character recommendation engine. Each line belongs to a word in an image containing text. It has information about the pixels of the boxes that have text inside (*left,right,top,bottom*), the text within those boxes (*Word_text*),\n",
    "and two id fields which give a unique identification for a file (*temp_id,doc_id*). Finally, it has a field *label* which is the classification of the word into a series of labels defined. This label is the class that a machine learning algorithm would eventually be trained to predict. **We are not asking for implementing a machine learning algorithm**. The task at hand is more about the feature engineering side of machine learning. We consider the **computational efficiency** and **clean code** as the main code value topics so please keep that in mind for the whole exercise.\n",
    "\n",
    "There is only one general rule we ask for:\n",
    "\n",
    "**The code generated must be compatible wit the Pipeline module of scikit-learn.** This means, the transformations done to the *csv* file, read as a *pd.DataFrame*, need to be coded so that at the end, running a pipeline.fit_transform() method should give the expected result.\n",
    "\n",
    "As a final question, we would like you to ask the question on why do we require the sci-kit learn compatibility of the code.\n",
    "\n",
    "Although not explicitly required, **the use of DOCSTRING descriptions** will be highly appreciated.\n",
    "\n",
    "We are aware that in some of the feature engineering tasks required, due to problems of the OCR output, a general feature engineering that works perfect in every single case might not be possible. So we are not asking for the perfect feature engineering, but the most general possible.\n",
    "\n",
    "**Do not hesitate on reaching us if you have doubts in any instructions.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Small tip\n",
    "\n",
    "To do things easier, generate your own conda environment and install the following packages:\n",
    "```\n",
    "conda create -n camelot-interview python=3.6\n",
    "conda activate camelot-interview \n",
    "pip install pandas\n",
    "pip install scikit-learn\n",
    "pip install numpy\n",
    "pip install quantulum3\n",
    "pip install tqdm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import pandas as pd    \n",
    "import glob\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from quantulum3 import parser\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading the data.\n",
    "\n",
    "You can just copy and paste this section of the code. \n",
    "\n",
    "**BONUS TASK** a more effective way of reading the data would be appreciated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(path):\n",
    "    files = glob.glob(path+\"template7_*.csv\")\n",
    "    li = []\n",
    "    for filename in files[:]:\n",
    "        df = pd.read_csv(filename, index_col=None, header=0)\n",
    "        li.append(df)\n",
    "\n",
    "    df_test = pd.concat(li, axis=0, ignore_index=True)\n",
    "    df_test.drop(df_test.columns[0], axis=1, inplace=True)\n",
    "    \n",
    "    return df_test\n",
    "df_test = load_test_data(\"training_data_new/predictions_labels/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see the list of labels present inthe dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['none', 'date', 'un_label', 'un_value', 'customer_name',\n",
       "       'customer_street', 'customer_zip_city', 'customer_country',\n",
       "       'packing_type_label', 'packing_type_value', 'material_id',\n",
       "       'volume_label', 'volume_value', 'volume_unit', 'volume_unc',\n",
       "       'weight_label', 'weight_value', 'height_unit', 'height_label',\n",
       "       'diameter_label', 'height_value', 'diameter_value', 'height_unc',\n",
       "       'diameter_unc', 'diameter_unit', 'wt_top_label', 'wt_body_label',\n",
       "       'wt_bottom_label', 'delivery_label', 'delivery_value',\n",
       "       'color_label', 'color_value', 'customer_id_label', 'customer_id',\n",
       "       'weight_unit', 'width_label', 'length_value', 'width_value',\n",
       "       'length_unit', 'width_unit', 'length_unc', 'Width_unc',\n",
       "       'weight_unc', 'wt_top_unit', 'wt_top_value', 'wt_body_value',\n",
       "       'wt_bottom_value', 'wt_body_unit', 'wt_top_unc', 'wt_body_unc',\n",
       "       'wt_bottom_unc', 'wt_bottom_unit', 'length_label'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[\"label\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature engineering\n",
    "\n",
    "\n",
    "**MAIN TASK**: Put each function into its own class and group all of them together in one main feature engineering class that you will use to generate the DataFrame expected in the result\n",
    "\n",
    "This is the main part of the task. In this implementation, as you can see, there is a single function called feature engineering that has defined internally several other functions which is bad practice. We finally call *feature_engineering()*.  \n",
    "\n",
    "As stated above, we would like to have each of these functions in its own class, ideally they should be compatible with sklearn.Pipeline as you will see in part 3. Below we give a list of the functions and transformations that we would need:\n",
    "\n",
    "* **is_date** - will generate a column called is_date that is 1 when the text is compatible with a date and 0 when not\n",
    "* **Extra step**: it is quite common that OCR engines confuse l (the low case L) with 1, when appearing together. This is critical when you try to find out numbers and units, as is the case in the context of this exercise.\n",
    "\n",
    "**BONUS TASK** Please write a function, again compatible with sklearn pipeline, that changes 1 for l, when 1 appears alone in the text field.\n",
    "* **is_number** - Generates boolean column that gives 1 when the *Word_text* field is a number\n",
    "* **is_percent** - Boolean column stating if a percentage value is present in *Word_text*\n",
    "* **unit_type** - The context of this exercise is to find out on a material specification sheet, values of different measurements, dimensions and their units in order to automate the data processing from scanned documents to DB systems. To facilitate the life to the algorithm, we thought that telling the kind of unit in a row of text might help. Here, we generate a column called unit_type that says, if a unit is present on the text box, whether it is a unit of volume, mass or length. \n",
    "\n",
    "**BONUS TASK** *Consider improving the implementation below by using a more robust alternative. For example, a package like quantulum 3, which is intended to detect units and magnitudes. If quantulum is used, make sure of taking an special look to those lines with mass units on them. Is there something that is not working and could be made to work by tweaking something in quantulum3 package?.* **NOTE IMPORTANT** Do not use the column label to detect if a unit is present on the dataset row. This information would not exist in the production environment.\n",
    "* **is_UN** UN numbers are approval numbers for dangerous materials. These have easily recognizable patterns. This function creates a boolean column that says whether a UN value is present.\n",
    "* **y_mean** (Not in the code below) Add a column that gives the mean between the bottom and top columns for every line.\n",
    "* **row_unit_type** If a unit is present in the same line of text (consider a line as that with similar y_mean values) then, asign **unit_type** to all the elements in that line. \n",
    "\n",
    "**BONUS TASK** Note that row_unit_type process can be massively improved compared to the implementation shown here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define own units because we can be sure they exists + we can define categories\n",
    "# Pre processing needed for the unit_type\n",
    "units = [\"m\",\"mm\",\"cm\",\"µm\"]\n",
    "units.extend([\"l\",\"ml\",\"m3\"])\n",
    "units.extend([\"g\",\"gr\", \"kg\"])\n",
    "units\n",
    "\n",
    "unit_to_category = dict() #to check what units belongs to what category\n",
    "                          #0 would be none\n",
    "unit_to_category[\"m\"] = 1 #1 would be length\n",
    "unit_to_category[\"mm\"] = 1\n",
    "unit_to_category[\"cm\"] = 1\n",
    "unit_to_category[\"µm\"] = 1\n",
    "#inches\n",
    "unit_to_category[\"l\"] = 2 #2 would be volume\n",
    "unit_to_category[\"ml\"] = 2 \n",
    "unit_to_category[\"m3\"] = 2\n",
    "#gallons\n",
    "unit_to_category[\"g\"] = 3 #3 would be weight\n",
    "unit_to_category[\"gr\"] = 3 \n",
    "unit_to_category[\"kg\"] = 3\n",
    "\n",
    "import re\n",
    "unit_to_cat=dict()\n",
    "unit_to_cat[\"volume\"] = 1\n",
    "unit_to_cat[\"mass\"] = 2\n",
    "unit_to_cat[\"length\"] = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feature_engineering(df):\n",
    "    \n",
    "    def is_date(string):\n",
    "        try:\n",
    "            return 1 if re.match(r\"\\d{4}-\\d{2}-\\d{2}\",string) is not None else 0\n",
    "        except TypeError:\n",
    "            print(string)\n",
    "    df[\"is_date\"] = df.Word_text.apply(is_date).astype(\"boolean\")\n",
    "    \n",
    "    def is_number(string):#try to convert to numpy float\n",
    "        try:\n",
    "            np.array(string,dtype=\"float\")\n",
    "            return 1\n",
    "        except ValueError:\n",
    "            return 0\n",
    "    df[\"is_number\"] = df.Word_text.apply(is_number).astype(\"boolean\")\n",
    "    \n",
    "    #def is_number_with_character(string):\n",
    "    #    return 1 if re.match(r\"\\d+\\w{0,2}\",string) is not None else 0\n",
    "    #df[\"is_number_with_character\"] = df.Word_text.apply(is_number_with_character).astype(\"boolean\")\n",
    "    \n",
    "    def is_number_with_percent(string):\n",
    "        return 1 if re.match(r\"\\d+%\",string) is not None else 0\n",
    "    df[\"is_number_with_percent\"] = df.Word_text.apply(is_number_with_percent).astype(\"boolean\")\n",
    "    \n",
    "    def is_a_unit(string): #if unit type != 0 then it is already a unit\n",
    "        return 1 if string.lower() in units else 0\n",
    "    df[\"is_a_unit\"] = df.Word_text.apply(is_a_unit).astype(\"boolean\")\n",
    "    \n",
    "    def unit_type(string):\n",
    "        if string.lower() in units:\n",
    "            return unit_to_category[string.lower()]\n",
    "        return 0\n",
    "    df[\"unit_type\"] = df.Word_text.apply(unit_type).astype(\"int8\")\n",
    "    \n",
    "        \n",
    "    \n",
    "    UN_REGEX = r\"UN([a-zA-Z0-9.]+/)+[a-zA-Z0-9]+\"\n",
    "    def is_UN(string):\n",
    "        return 1 if re.match(UN_REGEX, string.strip()) is not None else 0\n",
    "    df[\"is_UN\"] = df.Word_text.apply(is_UN).astype(\"boolean\")\n",
    "    \n",
    "    # Below there is the function that tranforms the \"1\" cointained in a character string in a \"l\"\n",
    "    \n",
    "    def one_replace_l(string):\n",
    "        for item in df[\"Word_text\"]:\n",
    "            if \"1\" in item:\n",
    "                if sum(c.isdigit() for c in item) == 1:\n",
    "                    item = item.replace(\"1\", \"l\")\n",
    "            \n",
    "    return df\n",
    "df_test = feature_engineering(df_test)\n",
    "\n",
    "df_test[\"y_mean\"] = (df_test[\"top\"] + df_test[\"bottom\"]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "finding and labeling lines...:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "df_test[\"row_unit_type\"] = 0\n",
    "y_delta = 10\n",
    "\n",
    "for template_id in tqdm(df_test.temp_id.unique(), desc=\"finding and labeling lines...\"):\n",
    "    template_df = df_test[df_test.temp_id == template_id]\n",
    "    for doc_id in template_df.doc_id.unique():\n",
    "        doc_df = template_df[template_df.doc_id==doc_id]\n",
    "        for i in range(len(doc_df)):\n",
    "            rows = doc_df[(abs(doc_df.top-doc_df.iloc[i].top) < y_delta) &\\\n",
    "                          (abs(doc_df.bottom-doc_df.iloc[i].bottom) < y_delta)]\n",
    "            if len(rows.unit_type.unique()) > 1:\n",
    "                for j in rows.index:\n",
    "                    df_test.loc[j,\"row_unit_type\"] = np.delete(rows.unit_type.unique(),0)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example of the dataframe expected as return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapting the data frame to the pipeline\n",
    "\n",
    "**MAIN TASK** In this part of the code, you will make sure that the feature engineering class is compatible \n",
    "Sklearn Pipeline module. Scikit-learn is one of the most popular library in the implementation of ML algorithm in python. It is widely used because of its built in features that allow the user to model and manage data also in the pre-processing stages.\n",
    "\n",
    "An example implementation can be found here https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "\n",
    "**BONUS TASK** A lot of optimization can be made in preprocessing to make the machine LEARNING task easy, one example is one hot encoding which changes categorical variables to boolean format. other methods like rescaling, normalization and Standardization can also be benificial but must be used with care. Explain what extra methods you would use on this dataset to make the machine learning task easier, even better just code it in the sklearn compatible class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe_style = OneHotEncoder()\n",
    "oe_results = oe_style.fit_transform(df_test[[\"Word_text\", \"unit_type\", \"label\", 'temp_id', \n",
    "                                             'doc_id', 'is_date', 'is_number', 'is_number_with_percent',\n",
    "                                              'is_a_unit','unit_type', 'is_UN', 'y_mean', 'row_unit_type']])\n",
    "pd.DataFrame(oe_results.toarray()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
